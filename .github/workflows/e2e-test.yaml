---
name: e2e
# yamllint disable rule:line-length
# yamllint disable-line rule:truthy
on:
  pull_request:
    branches:
      - main
      - release-*

# cancel the in-progress workflow when PR is refreshed.
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'pull_request' && github.head_ref || github.sha }}
  cancel-in-progress: true

env:
  TAG: test

permissions:
  contents: read

jobs:
  test:
    name: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 1
      - uses: actions/setup-go@7a3fe6cf4cb3a834922a1244abfce67bcef6a0c5 # v6.2.0
        with:
          go-version-file: go.mod
      - name: Install cri-dockerd
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          ARCH="$(go env GOARCH)"
          curl -LO "https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.21/cri-dockerd_0.3.21.3-0.ubuntu-focal_${ARCH}.deb"
          sudo dpkg -i "cri-dockerd_0.3.21.3-0.ubuntu-focal_${ARCH}.deb"
          rm -f "cri-dockerd_0.3.21.3-0.ubuntu-focal_${ARCH}.deb"

      - name: Setup Minikube
        uses: medyagh/setup-minikube@latest
        with:
          # lock minikube version to ensure release branch CI doesn't fail when latest minikube no
          # longer supports older k8s version we lock to
          minikube-version: "1.38.0"
          kubernetes-version: "1.31.0"
          driver: none
          container-runtime: docker
          memory: 6g
          cpus: 2
          addons: ingress
          cni: calico

      - name: install deps
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: test/e2e/scripts/ceph/github-action-helper.sh install_deps

      - name: print k8s cluster status
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: test/e2e/scripts/ceph/github-action-helper.sh print_k8s_cluster_status

      - name: use local disk into two partitions
        run: |
          test/e2e/scripts/ceph/github-action-helper.sh use_local_disk
          BLOCK=$(test/e2e/scripts/ceph/github-action-helper.sh find_extra_block_dev)
          export BLOCK="/dev/${BLOCK}"
          test/e2e/scripts/ceph/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
          sudo lsblk

      - name: build sidecar image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: build/Containerfile.sidecar
          push: false
          tags: quay.io/csiaddons/k8s-sidecar:${{ env.TAG }}

      - name: push sidecar image to minikube
        run: >
          minikube
          image load quay.io/csiaddons/k8s-sidecar:${{ env.TAG }}

      - name: checkout rook release
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: test/e2e/scripts/ceph/github-action-helper.sh checkout_rook_release "v1.19.1"

      - name: deploy ceph clusters
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          test/e2e/scripts/ceph/github-action-helper.sh deploy_first_ceph_cluster
          test/e2e/scripts/ceph/github-action-helper.sh deploy_second_ceph_cluster

      - name: enable mirroring between clusters
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          test/e2e/scripts/ceph/github-action-helper.sh enable_mirroring_cluster rook-ceph rook-ceph-secondary

      - name: Build controller container image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile
          push: false
          tags: quay.io/csiaddons/k8s-controller:${{ env.TAG }}

      - name: push controller image to minikube
        run: >
          minikube
          image load quay.io/csiaddons/k8s-controller:${{ env.TAG }}

      - name: Deploy the controller and CRDs
        run: make deploy TAG=${{ env.TAG }}

      - name: Wait for controller pod creation
        run: >
          kubectl
          -n csi-addons-system
          wait pods
          -l app.kubernetes.io/name=csi-addons
          --for=create
          --timeout=5m

      - name: Wait for running controller pod
        run: >
          kubectl
          -n csi-addons-system
          wait pods
          -l app.kubernetes.io/name=csi-addons
          --for=condition=Ready=True
          --timeout=5m

      - name: Run e2e tests
        run: |
          export KUBECONFIG="$HOME"/.kube/config
          echo "$HOME"/.kube/config
          make test-e2e

      - name: Capture debug info on failure
        if: failure()
        shell: bash --noprofile --norc -eo pipefail -x {0}
        run: |
          echo "=== Capturing debug information ==="

          echo "=== Pods in rook-ceph namespace ==="
          kubectl get po -n rook-ceph -o wide || true

          echo "=== Pods in rook-ceph-secondary namespace ==="
          kubectl get po -n rook-ceph-secondary -o wide || true

          echo "=== Pod descriptions in rook-ceph namespace ==="
          for pod in $(kubectl get po -n rook-ceph -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- Describing pod: $pod ---"
            kubectl describe pod "$pod" -n rook-ceph || true
          done

          echo "=== Pod descriptions in rook-ceph-secondary namespace ==="
          for pod in $(kubectl get po -n rook-ceph-secondary -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- Describing pod: $pod ---"
            kubectl describe pod "$pod" -n rook-ceph-secondary || true
          done

          echo "=== Pod logs in rook-ceph namespace ==="
          for pod in $(kubectl get po -n rook-ceph -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- Logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          echo "=== Pod logs in rook-ceph-secondary namespace ==="
          for pod in $(kubectl get po -n rook-ceph-secondary -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- Logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph-secondary --all-containers=true --tail=100 || true
          done

          echo "=== CSI logs in rook-ceph namespace ==="
          for pod in $(kubectl get po -n rook-ceph -l app=csi-rbdplugin -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- CSI RBD plugin logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          for pod in $(kubectl get po -n rook-ceph -l app=csi-cephfsplugin -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- CSI CephFS plugin logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          for pod in $(kubectl get po -n rook-ceph -l app=csi-rbdplugin-provisioner -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- CSI RBD provisioner logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          for pod in $(kubectl get po -n rook-ceph -l app=csi-cephfsplugin-provisioner -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- CSI CephFS provisioner logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          echo "--- csi addons resources ---"
          kubectl get encryptionkeyrotationjobs,encryptionkeyrotationcronjobs,reclaimspacecronjobs,reclaimspacejobs,networkfences,networkfenceclasses -A -oyaml

          echo "--- csi addons controller logs ---"
          for pod in $(kubectl get po -n csi-addons-system -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true); do
            echo "--- Logs for pod: $pod ---"
            kubectl logs "$pod" -n rook-ceph --all-containers=true --tail=100 || true
          done

          echo "=== Debug information capture complete ==="
